version: "3.9"

services:
  backend:
    build: ./api                 # usa api/Dockerfile
    ports:
      - "8000:8000"              # http://localhost:8000
    environment:
      # --- config que usas en tu código ---
      DB_PATH: /data/data.db     # ruta SQLite dentro del contenedor
      MODEL_CACHE: /models       # dir para caché/pesos si los descargas
      CORS_ORIGINS: "*"          # CORS abierto para el front (hackatón)
      TF_ENABLE_ONEDNN_OPTS: "0" # tus flags TF
      TF_CPP_MIN_LOG_LEVEL: "3"
      OLLAMA_URL: http://ollama:11434/api/generate
    volumes:
      - api_data:/data           # persiste SQLite fuera del contenedor
      - models_cache:/models     # persiste caché/pesos
    depends_on:
      - ollama
    restart: unless-stopped      # si se cae, se levanta solo (útil en demo)

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"            # opcional: solo si quieres llamar desde tu host para probar
    volumes:
      - ollama_models:/root/.ollama   # persiste los modelos descargados
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:11434/api/tags" ]
      interval: 10s
      timeout: 5s
      retries: 20
    restart: unless-stopped

  frontend:
    build: ./Front
    ports:
      - "8080:80"
    depends_on:
      - backend

  frontend2:
    build: ./Front2
    ports:
      - "8081:3000"                 # front 2 en http://localhost:8081
    environment:
      # Si tu app lee una API pública desde el cliente:
      # NEXT_PUBLIC_API_URL: "http://localhost:8000"
      NODE_ENV: production
    depends_on:
      - backend
volumes:
  api_data:
  models_cache:
  ollama_models: